{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate, Bidirectional \n",
    "from keras import Model\n",
    "from keras import optimizers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.read_csv('Daily Volatility Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_full[[\"Volume\", \"Return\", \"Return_Squared\", \"EMAF\", \"Daily Volatility\"]]\n",
    "Y = data_full[\"target\"]\n",
    "data_set = data_full[[\"Volume\", \"Return\", \"Return_Squared\", \"EMAF\", \"Daily Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitlimit = int(len(data_set)*0.8)\n",
    "training_features, test_data = data_set[:splitlimit], data_set[splitlimit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler1 = MinMaxScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "training_data_features_scaled = scaler1.fit_transform(training_features[[\"Volume\", \"Return\", \"Return_Squared\", \"EMAF\", \"Daily Volatility\"]])\n",
    "dataset_scaled = scaler2.fit_transform(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = []\n",
    "\n",
    "backcandles = 10\n",
    "\n",
    "for j in range(5):\n",
    "    Z.append([])\n",
    "    for i in range(backcandles, training_data_features_scaled.shape[0]):\n",
    "        Z[j].append(training_data_features_scaled[i-backcandles:i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.moveaxis(Z, [0], [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, yi = np.array(Z), np.array(dataset_scaled[backcandles-1:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final = np.reshape(yi,(len(yi),1))\n",
    "y_final = y_final[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Search and Walk-Forward Cross-Validation\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Activation\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "import numpy as np\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "\n",
    "def create_model(units=80):\n",
    "    lstm_input = Input(shape=(backcandles, 5), name='lstm_input')\n",
    "    inputs = LSTM(units, name='first_layer')(lstm_input)\n",
    "    inputs = Dense(1, name='dense_layer')(inputs)\n",
    "    output = Activation('sigmoid', name='output')(inputs)\n",
    "    model = Model(inputs=lstm_input, outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#hyperparameter space\n",
    "param_dist = {\n",
    "    'units': randint(50, 150),  \n",
    "    'batch_size': [16, 32, 64],  \n",
    "    'epochs': randint(10,30), \n",
    "}\n",
    "\n",
    "\n",
    "best_score = -np.inf  # Initialize best score\n",
    "best_params = None  # Initialize best parameters\n",
    "best_model_path = \"best_model.h5\"  # Path to save the best model\n",
    "\n",
    "n_iter = 200  # Number of iterations for random search\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "best_score = -np.inf \n",
    "best_params = None  \n",
    "\n",
    "for params in ParameterSampler(param_dist, n_iter=n_iter):\n",
    "    fold_scores = []  # Store scores for each fold\n",
    "    \n",
    "    for train_index, test_index in tscv.split(Z):\n",
    "        # Split data\n",
    "        X_train_fold, X_val_fold = Z[train_index], Z[test_index]\n",
    "        y_train_fold, y_val_fold = y_final[train_index], y_final[test_index]\n",
    "        \n",
    "        # Create model\n",
    "        model = create_model(units=params['units'])\n",
    "        \n",
    "        # Compile model with the chosen hyperparameters\n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train_fold, y_train_fold, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        _, score = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "        fold_scores.append(score)\n",
    "    \n",
    "    # Compute the average score across all folds\n",
    "    avg_score = np.mean(fold_scores)\n",
    "    \n",
    "    # If the current model's score is better, update best score, parameters, and save the model\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_params = params\n",
    "        \n",
    "        # Save the current best model\n",
    "        model.save(best_model_path)\n",
    "        print(f\"New best model saved with score: {avg_score}\")\n",
    "        \n",
    "# After the search\n",
    "print(f\"Best Score: {best_score}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = load_model(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstructing test data \n",
    "\n",
    "training_data_features_scaled = scaler1.transform(test_data[[\"Volume\", \"Return\", \"Return_Squared\", \"EMAF\", \"Daily Volatility\"]])\n",
    "training_dataset_scaled = scaler2.transform(test_data)\n",
    "\n",
    "T = []\n",
    "\n",
    "backcandles = 10\n",
    "\n",
    "for j in range(5):\n",
    "    T.append([])\n",
    "    for i in range(backcandles, training_data_features_scaled.shape[0]):\n",
    "        T[j].append(training_data_features_scaled[i-backcandles:i, j])\n",
    "        \n",
    "        \n",
    "T = np.moveaxis(T, [0], [2])\n",
    "T, yi_test = np.array(T), np.array(training_dataset_scaled[backcandles-1:, -1])\n",
    "y_final_test = np.reshape(yi_test,(len(yi_test),1))\n",
    "y_final_test = y_final_test[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = best_model.predict(T)\n",
    "test_predicted_classes = (test_predictions > 0.5).astype(int)\n",
    "validation_predictions = best_model.predict(Z)\n",
    "validation_predicted_classes = (validation_predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_frame = test_data[[\"target\"]].tail(955- backcandles +1)\n",
    "y_frame = y_frame.iloc[:-1]\n",
    "y_frame['predicted'] = test_predicted_classes\n",
    "y_frame_v = training_features[[\"target\"]].tail(3817- backcandles +1)\n",
    "y_frame_v = y_frame_v.iloc[:-1]\n",
    "y_frame_v['predicted'] = validation_predicted_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out-of-sample confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_frame['predicted'], y_frame['target'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Calculate ROC curve, AUC, and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_frame['target'], test_predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Print the AUC\n",
    "print(f\"AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "cmap = cm.get_cmap('viridis')  # Choose a colormap\n",
    "\n",
    "# The scatter plot for the ROC points\n",
    "sc = plt.scatter(fpr, tpr, c=thresholds, cmap=cmap, edgecolor='none', s =70)\n",
    "\n",
    "# Optionally, print the thresholds alongside FPR and TPR for inspection\n",
    "for f, t, thresh in zip(fpr, tpr, thresholds):\n",
    "    print(f\"Threshold: {thresh:.2f}, 1-Specificity: {f:.2f}, Sensitivity: {t:.2f}\")\n",
    "\n",
    "# Plotting the ROC curve\n",
    "plt.plot(fpr, tpr, color='black', lw=1, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.gca().tick_params(axis='x', labelsize=15)\n",
    "plt.gca().tick_params(axis='y', labelsize=15)\n",
    "plt.xlabel('1-Specificity', fontsize=20)\n",
    "plt.ylabel('Sensitivity', fontsize=20)\n",
    "\n",
    "\n",
    "# Adding colorbar with custom font for the label\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('Threshold', size=18)\n",
    "cbar.ax.tick_params(labelsize=15)\n",
    "\n",
    "plt.legend(loc=\"lower right\", fontsize= 15)\n",
    "#plt.savefig('ROC LSTM.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss_model",
   "language": "python",
   "name": "diss_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
