{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate, Bidirectional \n",
    "from keras import Model\n",
    "from keras import optimizers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.read_csv('Intraday Volatility Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_full[[\"Volume\", \"Return\", \"Return_Squared\", \"EMAF\", \"Daily Volatility\"]]\n",
    "Y = data_full[\"target\"]\n",
    "data_set = data_full[[\"Date\", \"Volume\", \"Return\", \"Return_Squared\", \"EMAF\", \"Daily Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitlimit = int(len(data_set)*0.8)\n",
    "training_features, test_data = data_set[:splitlimit], data_set[splitlimit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to remove all outliers from the dataset\n",
    "\n",
    "training_features[\"volatility_rolling_median\"] = training_features[\"Daily Volatility\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "training_features[\"return_squared_rolling_median\"] = training_features[\"Return_Squared\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "training_features[\"return_rolling_median\"] = training_features[\"Return\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "training_features[\"EMAF_rolling_median\"] = training_features[\"EMAF\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "training_features[\"volume_rolling_median\"] = training_features[\"Volume\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "\n",
    "training_features[\"volatility minus median\"] = (training_features[\"Daily Volatility\"] - training_features[\"volatility_rolling_median\"]).abs()\n",
    "training_features[\"return_squared minus median\"] = (training_features[\"Return_Squared\"] - training_features[\"return_squared_rolling_median\"]).abs()\n",
    "training_features[\"return minus median\"] = (training_features[\"Return\"] - training_features[\"return_rolling_median\"]).abs()\n",
    "training_features[\"EMAF minus median\"] = (training_features[\"EMAF\"] - training_features[\"EMAF_rolling_median\"]).abs()\n",
    "training_features[\"volume minus median\"] = (training_features[\"Volume\"] - training_features[\"volume_rolling_median\"]).abs()\n",
    "\n",
    "volatility_outliers_removed = training_features[~(training_features['volatility minus median'] > 5 * training_features['volatility minus median'].median())]\n",
    "all_outliers_removed = volatility_outliers_removed[~(volatility_outliers_removed['return_squared minus median'] > 5 * volatility_outliers_removed['return_squared minus median'].median())]\n",
    "all_outliers_removed = all_outliers_removed[~(all_outliers_removed['return minus median'] > 5 * volatility_outliers_removed['return minus median'].median())]\n",
    "all_outliers_removed = all_outliers_removed[~(all_outliers_removed['EMAF minus median'] > 5 * volatility_outliers_removed['EMAF minus median'].median())]\n",
    "all_outliers_removed = all_outliers_removed[~(all_outliers_removed['volume minus median'] > 5 * volatility_outliers_removed['volume minus median'].median())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cleaned = all_outliers_removed[[\"Volume\", \"Return\", \"Return_Squared\", \"EMAF\", \"Daily Volatility\"]]\n",
    "Y_cleaned = all_outliers_removed[\"target\"]\n",
    "data_set_cleaned = all_outliers_removed[[\"Volume\", \"Return\", \"Return_Squared\", \"EMAF\", \"Daily Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "training_data_features_scaled = scaler.fit_transform(X_cleaned)\n",
    "data_set_scaled = scaler.fit_transform(data_set_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = []\n",
    "\n",
    "backcandles = 10\n",
    "\n",
    "for j in range(5):\n",
    "    Z.append([])\n",
    "    for i in range(backcandles, training_data_features_scaled.shape[0]):\n",
    "        Z[j].append(training_data_features_scaled[i-backcandles:i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.moveaxis(Z, [0], [2])\n",
    "Z, yi = np.array(Z), np.array(data_set_scaled[backcandles-1:, -1])\n",
    "y_final = np.reshape(yi,(len(yi),1))\n",
    "y_final = y_final[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Search and Walk-Forward Cross-Validation\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Activation\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "import numpy as np\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "\n",
    "def create_model(units=80):\n",
    "    lstm_input = Input(shape=(backcandles, 5), name='lstm_input')\n",
    "    inputs = LSTM(units, name='first_layer')(lstm_input)\n",
    "    inputs = Dense(1, name='dense_layer')(inputs)\n",
    "    output = Activation('sigmoid', name='output')(inputs)\n",
    "    model = Model(inputs=lstm_input, outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    'units': randint(50, 150),  \n",
    "    'batch_size': [16, 32, 64],  \n",
    "    'epochs': randint(10,30), \n",
    "}\n",
    "\n",
    "\n",
    "best_score = -np.inf  \n",
    "best_params = None  \n",
    "best_model_path = \"best_model.h5\" \n",
    "n_iter = 1  \n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "best_score = -np.inf \n",
    "best_params = None  \n",
    "\n",
    "for params in ParameterSampler(param_dist, n_iter=n_iter):\n",
    "    fold_scores = []  \n",
    "    \n",
    "    for train_index, test_index in tscv.split(Z):\n",
    "       \n",
    "    \n",
    "        X_train_fold, X_val_fold = Z[train_index], Z[test_index]\n",
    "        \n",
    "        y_train_fold, y_val_fold = y_final[train_index], y_final[test_index]\n",
    "        \n",
    "        model = create_model(units=params['units'])\n",
    "        \n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "       \n",
    "        model.fit(X_train_fold, y_train_fold, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1)\n",
    "        \n",
    "        _, score = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "        \n",
    "        fold_scores.append(score)\n",
    "    \n",
    "\n",
    "    avg_score = np.mean(fold_scores)\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_params = params\n",
    "        model.save(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data[[\"Volume\", \"Return\", \"Return_Squared\", \"EMAF\", \"Daily Volatility\"]]\n",
    "Y_test = test_data[\"target\"]\n",
    "test_dataset = test_data[[\"Volume\", \"Return\", \"Return_Squared\", \"EMAF\", \"Daily Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling test data\n",
    "test_scaled = scaler.fit_transform(test_dataset)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstructing test data \n",
    "\n",
    "\n",
    "T = []\n",
    "\n",
    "backcandles = 10\n",
    "\n",
    "for j in range(5):\n",
    "    T.append([])\n",
    "    for i in range(backcandles, X_test_scaled.shape[0]):\n",
    "        T[j].append(X_test_scaled[i-backcandles:i, j])\n",
    "        \n",
    "        \n",
    "T = np.moveaxis(T, [0], [2])\n",
    "T, yi_test = np.array(T), np.array(test_scaled[backcandles-1:, -1])\n",
    "y_final_test = np.reshape(yi_test,(len(yi_test),1))\n",
    "y_final_test = y_final_test[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm in sample \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "validation_predictions = best_model.predict(Z)\n",
    "validation_predicted_classes = (validation_predictions > 0.5).astype(int)\n",
    "dataframe_val = pd.DataFrame(y_final, columns = [\"target\"])\n",
    "dataframe_val[\"predicted\"] = validation_predicted_classes\n",
    "cm = confusion_matrix(dataframe_val['predicted'], dataframe_val['target'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm out of sample\n",
    "test_predictions = best_model.predict(T)\n",
    "test_predicted_classes = (test_predictions > 0.5).astype(int)\n",
    "dataframe = pd.DataFrame(y_final_test, columns = [\"target\"])\n",
    "dataframe[\"predicted\"] = test_predicted_classes\n",
    "cm = confusion_matrix(dataframe['predicted'], dataframe['target'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "false_pos_rate, true_pos_rate, thresholds = roc_curve(dataframe['target'], test_predictions)\n",
    "roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "\n",
    "\n",
    "# Making plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "cmap = cm.get_cmap('viridis')  \n",
    "sc = plt.scatter(fpr, tpr, c=thresholds, cmap=cmap, edgecolor='none', s =70)\n",
    "\n",
    "\n",
    "# Plot ROC\n",
    "plt.plot(fpr, tpr, color='black', lw=1)\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.gca().tick_params(axis='x', labelsize=15)\n",
    "plt.gca().tick_params(axis='y', labelsize=15)\n",
    "plt.xlabel('1-Specificity', fontsize=20)\n",
    "plt.ylabel('Sensitivity', fontsize=20)\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('Threshold', size=18)\n",
    "cbar.ax.tick_params(labelsize=15)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss_model",
   "language": "python",
   "name": "diss_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
