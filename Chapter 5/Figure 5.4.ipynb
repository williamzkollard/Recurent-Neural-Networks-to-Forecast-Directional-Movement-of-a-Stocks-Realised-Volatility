{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate, Bidirectional, GRU \n",
    "from keras import Model\n",
    "from keras import optimizers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.read_csv('Intrahour Volatility Dataset.csv')\n",
    "shuffled_data_full = data_full.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We undergo the outlier removal and data reconstruction for both data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test split\n",
    "splitlimit = int(len(data_full)*0.8)\n",
    "training_features_a, test_data_a = shuffled_data_full[:splitlimit], shuffled_data_full[splitlimit:]\n",
    "\n",
    "training_features_b, test_data_b = data_full[:splitlimit], shuffled_data_full[splitlimit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_a = shuffled_data_full[[\"Return_Squared\", \"Hourly Volatility\"]]\n",
    "Y_a = shuffled_data_full[\"target\"]\n",
    "data_set_a = shuffled_data_full[[\"Return_Squared\", \"Hourly Volatility\", \"target\"]]\n",
    "\n",
    "X_b = data_full[[\"Return_Squared\", \"Hourly Volatility\"]]\n",
    "Y_b = data_full[\"target\"]\n",
    "data_set_b = data_full[[\"Return_Squared\", \"Hourly Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier Detection in training_data_features\n",
    "\n",
    "training_features_a[\"hourly_volatility_rolling_median\"] = training_features_a[\"Hourly Volatility\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "training_features_a[\"return_squared_rolling_median\"] = training_features_a[\"Return_Squared\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "training_features_a[\"volatility minus median\"] = (training_features_a[\"Hourly Volatility\"] - training_features_a[\"hourly_volatility_rolling_median\"]).abs()\n",
    "training_features_a[\"return minus median\"] = (training_features_a[\"Return_Squared\"] - training_features_a[\"return_squared_rolling_median\"]).abs()\n",
    "volatility_outliers_removed_a = training_features_a[~(training_features_a['volatility minus median'] > 5 * training_features_a['volatility minus median'].median())]\n",
    "both_outliers_removed_a = volatility_outliers_removed_a[~(volatility_outliers_removed_a['return minus median'] > 5 * volatility_outliers_removed_a['return minus median'].median())]\n",
    "\n",
    "\n",
    "training_features_b[\"hourly_volatility_rolling_median\"] = training_features_b[\"Hourly Volatility\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "training_features_b[\"return_squared_rolling_median\"] = training_features_b[\"Return_Squared\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "training_features_b[\"volatility minus median\"] = (training_features_b[\"Hourly Volatility\"] - training_features_b[\"hourly_volatility_rolling_median\"]).abs()\n",
    "training_features_b[\"return minus median\"] = (training_features_b[\"Return_Squared\"] - training_features_b[\"return_squared_rolling_median\"]).abs()\n",
    "volatility_outliers_removed_b = training_features_b[~(training_features_b['volatility minus median'] > 5 * training_features_b['volatility minus median'].median())]\n",
    "both_outliers_removed_b = volatility_outliers_removed_b[~(volatility_outliers_removed_b['return minus median'] > 5 * volatility_outliers_removed_b['return minus median'].median())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cleaned_a = both_outliers_removed_a[[\"Return_Squared\", \"Hourly Volatility\"]]\n",
    "Y_cleaned_a = both_outliers_removed_a[\"target\"]\n",
    "data_set_cleaned_a = both_outliers_removed_a[[\"Return_Squared\", \"Hourly Volatility\", \"target\"]]\n",
    "\n",
    "X_cleaned_b = both_outliers_removed_b[[\"Return_Squared\", \"Hourly Volatility\"]]\n",
    "Y_cleaned_b = both_outliers_removed_b[\"target\"]\n",
    "data_set_cleaned_b = both_outliers_removed_b[[\"Return_Squared\", \"Hourly Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_a = MinMaxScaler()\n",
    "training_data_features_scaled_a = scaler_a.fit_transform(X_cleaned_a)\n",
    "data_set_scaled_a = scaler_a.fit_transform(data_set_cleaned_a)\n",
    "\n",
    "scaler_b = MinMaxScaler()\n",
    "training_data_features_scaled_b = scaler_b.fit_transform(X_cleaned_b)\n",
    "data_set_scaled_b = scaler_b.fit_transform(data_set_cleaned_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstructing 2 sets of training data \n",
    "\n",
    "backcandles = 15\n",
    "\n",
    "Z_a = []\n",
    "\n",
    "for j in range(2):\n",
    "    Z_a.append([])\n",
    "    for i in range(backcandles, training_data_features_scaled_a.shape[0]):\n",
    "        Z_a[j].append(training_data_features_scaled_a[i-backcandles:i, j])\n",
    "        \n",
    "Z_a = np.moveaxis(Z_a, [0], [2])\n",
    "Z_a, yi_a = np.array(Z_a), np.array(data_set_scaled_a[backcandles-1:, -1])\n",
    "y_final_a = np.reshape(yi_a,(len(yi_a),1))\n",
    "y_final_a = y_final_a[1:]\n",
    "\n",
    "\n",
    "Z_b = []\n",
    "\n",
    "for j in range(2):\n",
    "    Z_b.append([])\n",
    "    for i in range(backcandles, training_data_features_scaled_b.shape[0]):\n",
    "        Z_b[j].append(training_data_features_scaled_b[i-backcandles:i, j])\n",
    "        \n",
    "Z_b = np.moveaxis(Z_b, [0], [2])\n",
    "Z_b, yi_b = np.array(Z_b), np.array(data_set_scaled_b[backcandles-1:, -1])\n",
    "y_final_b = np.reshape(yi_b,(len(yi_b),1))\n",
    "y_final_b = y_final_b[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM on shuffled data model\n",
    "lstm_input = Input(shape = (backcandles, 2), name = 'lstm_input')\n",
    "\n",
    "inputs = LSTM(80, name='first_layer')(lstm_input)\n",
    "inputs = Dense(1, name='dense_layer')(inputs)\n",
    "output = Activation('sigmoid', name = 'output')(inputs)\n",
    "\n",
    "\n",
    "model_a = Model(inputs = lstm_input, outputs = output)\n",
    "model_a.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "history_a = model_a.fit(x=Z_a, y=y_final_a, epochs = 30, validation_data = (Z_a, y_final_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM on standard data model\n",
    "lstm_input = Input(shape = (backcandles, 2), name = 'lstm_input')\n",
    "\n",
    "inputs = LSTM(80, name='first_layer')(lstm_input)\n",
    "inputs = Dense(1, name='dense_layer')(inputs)\n",
    "output = Activation('sigmoid', name = 'output')(inputs)\n",
    "\n",
    "model_b = Model(inputs = lstm_input, outputs = output)\n",
    "model_b.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "history_b = model_b.fit(x=Z_b, y=y_final_b, epochs = 30, validation_data = (Z_b, y_final_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FFNN on standard data model\n",
    "\n",
    "input_shape = len(training_data_features_scaled_b[0, :]) \n",
    "ff_input = Input(shape=input_shape, name='ff_input')\n",
    "inputs = Dense(80, activation='relu', name='first_layer')(ff_input)  \n",
    "outputs = Dense(1, activation='sigmoid', name='output')(inputs)\n",
    "\n",
    "\n",
    "model_c = Model(inputs=ff_input, outputs=outputs)\n",
    "model_c.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_c = model_c.fit(x=training_data_features_scaled_b, y=data_set_scaled_b[:, -1], epochs=30, validation_data=(training_data_features_scaled_b, data_set_scaled_b[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3))\n",
    "\n",
    "plt.plot(history_b.history['accuracy'], color = 'red', label='LSTM on Ordered Data')\n",
    "\n",
    "plt.plot(history_c.history['accuracy'], color = 'grey', label='FFNN on Ordered Data')\n",
    "\n",
    "plt.plot(history_a.history['accuracy'], color = 'black', label='LSTM on Shuffled Data')\n",
    "plt.gca().tick_params(axis='x', labelsize=15)\n",
    "plt.gca().tick_params(axis='y', labelsize=15) \n",
    "plt.xlabel('Epoch', size = 17)\n",
    "plt.ylabel('Training Accuracy', size = 17)\n",
    "plt.legend(fontsize = 13)\n",
    "plt.savefig('Shuffled Vs Ordered Models.jpg', format='jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss_model",
   "language": "python",
   "name": "diss_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
