{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate, Bidirectional, GRU \n",
    "from keras import Model\n",
    "from keras import optimizers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.read_csv('Intrahour Volatility Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_full[[\"Return_Squared\", \"Hourly Volatility\"]]\n",
    "Y = data_full[\"target\"]\n",
    "data_set = data_full[[\"Date\",\"Return_Squared\", \"Hourly Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test split\n",
    "splitlimit = int(len(data_full)*0.8)\n",
    "training_features, test_data = data_set[:splitlimit], data_set[splitlimit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier Detection in training_data_features\n",
    "\n",
    "training_features[\"hourly_volatility_rolling_median\"] = training_features[\"Hourly Volatility\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "training_features[\"return_squared_rolling_median\"] = training_features[\"Return_Squared\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "training_features[\"volatility minus median\"] = (training_features[\"Hourly Volatility\"] - training_features[\"hourly_volatility_rolling_median\"]).abs()\n",
    "training_features[\"return minus median\"] = (training_features[\"Return_Squared\"] - training_features[\"return_squared_rolling_median\"]).abs()\n",
    "volatility_outliers_removed = training_features[~(training_features['volatility minus median'] > 5 * training_features['volatility minus median'].median())]\n",
    "both_outliers_removed = volatility_outliers_removed[~(volatility_outliers_removed['return minus median'] > 5 * volatility_outliers_removed['return minus median'].median())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cleaned = both_outliers_removed[[\"Return_Squared\", \"Hourly Volatility\"]]\n",
    "Y_cleaned = both_outliers_removed[\"target\"]\n",
    "data_set_cleaned = both_outliers_removed[[\"Return_Squared\", \"Hourly Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "training_data_features_scaled = scaler.fit_transform(X_cleaned)\n",
    "data_set_scaled = scaler.fit_transform(data_set_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstructing training data \n",
    "\n",
    "Z = []\n",
    "\n",
    "backcandles = 15\n",
    "\n",
    "for j in range(2):\n",
    "    Z.append([])\n",
    "    for i in range(backcandles, training_data_features_scaled.shape[0]):\n",
    "        Z[j].append(training_data_features_scaled[i-backcandles:i, j])\n",
    "        \n",
    "Z = np.moveaxis(Z, [0], [2])\n",
    "Z, yi = np.array(Z), np.array(data_set_scaled[backcandles-1:, -1])\n",
    "y_final = np.reshape(yi,(len(yi),1))\n",
    "y_final = y_final[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OGRU Cell and Layer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "           \n",
    "class OGRUCell(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(OGRUCell, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.state_size = units  \n",
    "        self.output_size = units  \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.resetweights1 = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                  initializer='glorot_uniform', #this sets the initial weights from uniform dist. \n",
    "                                  name='resetweights1')\n",
    "        self.resetweights2 = self.add_weight(shape=(self.units, self.units),\n",
    "                                  initializer='orthogonal',\n",
    "                                  name='resetweights2')\n",
    "        self.updateweights1 = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  name='updateweights1')\n",
    "        self.updateweights2 = self.add_weight(shape=(self.units, self.units),\n",
    "                                  initializer='orthogonal',\n",
    "                                  name='updateweights2')\n",
    "        self.candweights1 = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 name='candweights1')\n",
    "        self.candweights2 = self.add_weight(shape=(self.units, self.units),\n",
    "                                 initializer='orthogonal',\n",
    "                                 name='candweights2')\n",
    "        self.bias_r = self.add_weight(shape=(self.units,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_r')\n",
    "        self.bias_z = self.add_weight(shape=(self.units,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_z')\n",
    "        self.bias_h = self.add_weight(shape=(self.units,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_h')\n",
    "\n",
    "    \n",
    "    def call(self, inputs, states):\n",
    "        prev_h = states[0]\n",
    "\n",
    "        r = tf.sigmoid(tf.matmul(inputs, self.resetweights1) + tf.matmul(prev_h, self.resetweights2) + self.bias_r)\n",
    "        \n",
    "        #changing the dimension of the reset gate to allow for matrix multiplication\n",
    "        r_reduced = tf.reduce_mean(r, axis=-1, keepdims=True) \n",
    "        r_reduced = tf.tile(r_reduced, [1, inputs.shape[-1]])\n",
    "        \n",
    "        #the new update gate, which now filters the input by the reset gate\n",
    "        z = tf.sigmoid(tf.matmul(inputs * r_reduced, self.updateweights1) + tf.matmul(prev_h, self.updateweights2) + self.bias_z)\n",
    "\n",
    "        n = tf.tanh(tf.matmul(inputs, self.candweights1) + tf.matmul(prev_h * r, self.candweights2) + self.bias_h)\n",
    "\n",
    "        h = (1 - z) * prev_h + z * n\n",
    "\n",
    "        return h, [h]\n",
    "\n",
    "\n",
    "# Producing OGRU layer \n",
    "class OGRU(tf.keras.layers.RNN):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.cell = OGRUCell(units, **kwargs)\n",
    "        super(OGRU, self).__init__(self.cell, **kwargs)\n",
    "\n",
    "# OGRU layer with 80 units\n",
    "ogru_layer = OGRU(units=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU model\n",
    "gru_input = Input(shape = (backcandles, 2), name = 'gru_input')\n",
    "\n",
    "inputs = GRU(80, name='first_layer')(gru_input)\n",
    "\n",
    "inputs = Dense(1, name='dense_layer')(inputs)\n",
    "\n",
    "output = Activation('sigmoid', name = 'output')(inputs)\n",
    "\n",
    "model = Model(inputs = gru_input, outputs = output)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "history = model.fit(x=Z, y=y_final, epochs = 50, validation_data = (Z, y_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OGRU model\n",
    "ogru_input = Input(shape = (backcandles, 2), name = 'ogru_input')\n",
    "\n",
    "inputs = ogru_layer(ogru_input)\n",
    "\n",
    "inputs = Dense(1, name='dense_layer')(inputs)\n",
    "\n",
    "output = Activation('sigmoid', name = 'output')(inputs)\n",
    "\n",
    "model2 = Model(inputs = ogru_input, outputs = output)\n",
    "\n",
    "model2.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "history2 = model2.fit(x=Z, y=y_final, epochs = 50, validation_data = (Z, y_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gru plot\n",
    "history.history.keys() \n",
    "plt.plot(history.history['loss'], color = 'red', label = \"Training Loss\")\n",
    "plt.plot(history.history['accuracy'], color = 'royalblue', label = \"Training Accuracy\")\n",
    "plt.xlabel('Number of Epochs', fontsize = 15)\n",
    "plt.ylabel('Accuracy', fontsize = 15)\n",
    "plt.gca().tick_params(axis='x', labelsize=12)\n",
    "plt.gca().tick_params(axis='y', labelsize=12)\n",
    "plt.legend(fontsize= 12)\n",
    "#plt.savefig('GRU.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ogru plot\n",
    "history2.history.keys() \n",
    "plt.plot(history3.history['loss'], color = 'red', label = \"Training Loss\")\n",
    "plt.plot(history3.history['accuracy'], color = 'royalblue', label = \"Training Accuracy\")\n",
    "plt.xlabel('Number of Epochs', fontsize = 15)\n",
    "plt.ylabel('Accuracy', fontsize = 15)\n",
    "plt.gca().tick_params(axis='x', labelsize=12)\n",
    "plt.gca().tick_params(axis='y', labelsize=12)\n",
    "plt.legend(fontsize= 12)\n",
    "#plt.savefig('MyGRU.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 6.3\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.plot(history.history['loss'], color = 'red', label='GRU Model')\n",
    "plt.plot(history2.history['loss'], color = 'blue', label='OGRU Model')\n",
    "plt.gca().tick_params(axis='x', labelsize=15)\n",
    "plt.gca().tick_params(axis='y', labelsize=15) \n",
    "plt.xlabel('Epoch', size = 17)\n",
    "plt.ylim(0.30, 0.65)\n",
    "plt.ylabel('Training Loss', size = 17)\n",
    "plt.legend(fontsize = 13)\n",
    "plt.savefig('GRU vs OGRU.jpg', format='jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This additional section provides the out-of-sample results of the GRU vs OGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data[[\"Return_Squared\", \"Hourly Volatility\"]]\n",
    "Y_test = test_data[\"target\"]\n",
    "test_dataset = test_data[[\"Return_Squared\", \"Hourly Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled = scaler.fit_transform(test_dataset)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = []\n",
    "\n",
    "backcandles = 15\n",
    "\n",
    "for j in range(2):\n",
    "    T.append([])\n",
    "    for i in range(backcandles, X_test_scaled.shape[0]):\n",
    "        T[j].append(X_test_scaled[i-backcandles:i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.moveaxis(T, [0], [2])\n",
    "T, yi_test = np.array(T), np.array(test_scaled[backcandles-1:, -1])\n",
    "y_final_test = np.reshape(yi_test,(len(yi_test),1))\n",
    "y_final_test = y_final_test[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gru out-of-sample\n",
    "test_predictions1 = model.predict(T)\n",
    "test_predicted_classes1 = (test_predictions1 > 0.5).astype(int)\n",
    "dataframe1 = pd.DataFrame(y_final_test, columns = [\"target\"])\n",
    "dataframe1[\"predicted\"] = test_predicted_classes1\n",
    "cm_gru = confusion_matrix(dataframe1['predicted'], dataframe1['target'])\n",
    "print(cm_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ogru out-of-sample\n",
    "test_predictions2 = model2.predict(T)\n",
    "test_predicted_classes2 = (test_predictions2 > 0.5).astype(int)\n",
    "dataframe2 = pd.DataFrame(y_final_test, columns = [\"target\"])\n",
    "dataframe2[\"predicted\"] = test_predicted_classes2\n",
    "cm_ogru = confusion_matrix(dataframe2['predicted'], dataframe2['target'])\n",
    "print(cm_ogru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss_model",
   "language": "python",
   "name": "diss_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
