{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate, Bidirectional, GRU \n",
    "from keras import Model\n",
    "from keras import optimizers\n",
    "from tensorflow import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.read_csv('Intrahour Volatility Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_full[[\"Return_Squared\", \"Hourly Volatility\"]]\n",
    "Y = data_full[\"target\"]\n",
    "data_set = data_full[[\"Date\",\"Return_Squared\", \"Hourly Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitlimit = int(len(data_full)*0.8)\n",
    "training_features, test_data = data_set[:splitlimit], data_set[splitlimit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier Detection in training_data_features\n",
    "\n",
    "training_features[\"hourly_volatility_rolling_median\"] = training_features[\"Hourly Volatility\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "training_features[\"return_squared_rolling_median\"] = training_features[\"Return_Squared\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "training_features[\"volatility minus median\"] = (training_features[\"Hourly Volatility\"] - training_features[\"hourly_volatility_rolling_median\"]).abs()\n",
    "training_features[\"return minus median\"] = (training_features[\"Return_Squared\"] - training_features[\"return_squared_rolling_median\"]).abs()\n",
    "volatility_outliers_removed = training_features[~(training_features['volatility minus median'] > 5 * training_features['volatility minus median'].median())]\n",
    "both_outliers_removed = volatility_outliers_removed[~(volatility_outliers_removed['return minus median'] > 5 * volatility_outliers_removed['return minus median'].median())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cleaned = both_outliers_removed[[\"Return_Squared\", \"Hourly Volatility\"]]\n",
    "Y_cleaned = both_outliers_removed[\"target\"]\n",
    "data_set_cleaned = both_outliers_removed[[\"Return_Squared\", \"Hourly Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "training_data_features_scaled = scaler.fit_transform(X_cleaned)\n",
    "data_set_scaled = scaler.fit_transform(data_set_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstructing training data\n",
    "Z = []\n",
    "\n",
    "backcandles = 15\n",
    "\n",
    "for j in range(2):\n",
    "    Z.append([])\n",
    "    for i in range(backcandles, training_data_features_scaled.shape[0]):\n",
    "        Z[j].append(training_data_features_scaled[i-backcandles:i, j])\n",
    "        \n",
    "Z = np.moveaxis(Z, [0], [2])    \n",
    "Z, yi = np.array(Z), np.array(data_set_scaled[backcandles-1:, -1])\n",
    "y_final = np.reshape(yi,(len(yi),1))\n",
    "y_final = y_final[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = Input(shape = (backcandles, 2), name = 'lstm_input')\n",
    "\n",
    "inputs = LSTM(80, name='first_layer')(lstm_input)\n",
    "\n",
    "inputs = Dense(1, name='dense_layer')(inputs)\n",
    "\n",
    "output = Activation('sigmoid', name = 'output')(inputs)\n",
    "\n",
    "model = Model(inputs = lstm_input, outputs = output)\n",
    "model.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "start_time = time.time()\n",
    "history = model.fit(x=Z, y=y_final, epochs = 20, validation_data = (Z, y_final))\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model on the test data\n",
    "training_accuracy = model.evaluate(Z)\n",
    "\n",
    "print(training_time)\n",
    "print(training_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_input = Input(shape = (backcandles, 2), name = 'gru_input')\n",
    "\n",
    "inputs = GRU(80, name='first_layer')(gru_input)\n",
    "\n",
    "inputs = Dense(1, name='dense_layer')(inputs)\n",
    "\n",
    "output = Activation('sigmoid', name = 'output')(inputs)\n",
    "\n",
    "model2 = Model(inputs = gru_input, outputs = output)\n",
    "model2.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "start_time2 = time.time()\n",
    "history2 = model2.fit(x=Z, y=y_final, epochs = 20, validation_data = (Z, y_final))\n",
    "end_time2 = time.time()\n",
    "training_time2 = end_time2 - start_time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()\n",
    "print(training_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data[[\"Return_Squared\", \"Hourly Volatility\"]]\n",
    "Y_test = test_data[\"target\"]\n",
    "test_dataset = test_data[[\"Return_Squared\", \"Hourly Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling test data\n",
    "test_scaled = scaler.fit_transform(test_dataset)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstructing test data\n",
    "T = []\n",
    "\n",
    "backcandles = 15\n",
    "\n",
    "for j in range(2):\n",
    "    T.append([])\n",
    "    for i in range(backcandles, X_test_scaled.shape[0]):\n",
    "        T[j].append(X_test_scaled[i-backcandles:i, j])\n",
    "        \n",
    "T = np.moveaxis(T, [0], [2])\n",
    "T, yi_test = np.array(T), np.array(test_scaled[backcandles-1:, -1])\n",
    "y_final_test = np.reshape(yi_test,(len(yi_test),1))\n",
    "y_final_test = y_final_test[1:]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions1 = model.predict(T)\n",
    "test_predicted_classes1 = (test_predictions1 > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm\n",
    "test_predictions1 = model.predict(T)\n",
    "test_predicted_classes1 = (test_predictions1 > 0.5).astype(int)\n",
    "dataframe = pd.DataFrame(y_final_test, columns = [\"target\"])\n",
    "dataframe[\"predicted\"] = test_predicted_classes1\n",
    "cm_lstm = confusion_matrix(dataframe['predicted'], dataframe['target'])\n",
    "print(cm_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gru\n",
    "test_predictions2 = model2.predict(T)\n",
    "test_predicted_classes2 = (test_predictions2 > 0.5).astype(int)\n",
    "dataframe2 = pd.DataFrame(y_final_test, columns = [\"target\"])\n",
    "dataframe2[\"predicted\"] = test_predicted_classes2\n",
    "cm_gru = confusion_matrix(dataframe2['predicted'], dataframe2['target'])\n",
    "print(cm_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss_model",
   "language": "python",
   "name": "diss_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
