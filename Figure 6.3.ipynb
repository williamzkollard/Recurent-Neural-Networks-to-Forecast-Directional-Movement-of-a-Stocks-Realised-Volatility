{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate, Bidirectional, GRU \n",
    "from keras import Model\n",
    "from keras import optimizers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.read_csv('Hourly Volatility Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_full[[\"Return_Squared\", \"Hourly Volatility\"]]\n",
    "Y = data_full[\"target\"]\n",
    "data_set = data_full[[\"Date\",\"Return_Squared\", \"Hourly Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test split\n",
    "splitlimit = int(len(data_full)*0.8)\n",
    "training_features, test_data = data_set[:splitlimit], data_set[splitlimit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier Detection in training_data_features\n",
    "\n",
    "training_features[\"hourly_volatility_rolling_median\"] = training_features[\"Hourly Volatility\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "training_features[\"return_squared_rolling_median\"] = training_features[\"Return_Squared\"].rolling(window=41, center=True, min_periods=1).median()\n",
    "training_features[\"volatility minus median\"] = (training_features[\"Hourly Volatility\"] - training_features[\"hourly_volatility_rolling_median\"]).abs()\n",
    "training_features[\"return minus median\"] = (training_features[\"Return_Squared\"] - training_features[\"return_squared_rolling_median\"]).abs()\n",
    "volatility_outliers_removed = training_features[~(training_features['volatility minus median'] > 5 * training_features['volatility minus median'].median())]\n",
    "both_outliers_removed = volatility_outliers_removed[~(volatility_outliers_removed['return minus median'] > 5 * volatility_outliers_removed['return minus median'].median())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cleaned = both_outliers_removed[[\"Return_Squared\", \"Hourly Volatility\"]]\n",
    "Y_cleaned = both_outliers_removed[\"target\"]\n",
    "data_set_cleaned = both_outliers_removed[[\"Return_Squared\", \"Hourly Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "training_data_features_scaled = scaler.fit_transform(X_cleaned)\n",
    "data_set_scaled = scaler.fit_transform(data_set_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstructing training data \n",
    "\n",
    "Z = []\n",
    "\n",
    "backcandles = 15\n",
    "\n",
    "for j in range(2):\n",
    "    Z.append([])\n",
    "    for i in range(backcandles, training_data_features_scaled.shape[0]):\n",
    "        Z[j].append(training_data_features_scaled[i-backcandles:i, j])\n",
    "        \n",
    "Z = np.moveaxis(Z, [0], [2])\n",
    "Z, yi = np.array(Z), np.array(data_set_scaled[backcandles-1:, -1])\n",
    "y_final = np.reshape(yi,(len(yi),1))\n",
    "y_final = y_final[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OGRU Cell and Layer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "           \n",
    "class OGRUCell(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(OGRUCell, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.state_size = units  \n",
    "        self.output_size = units  \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.Wr = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  name='Wr')\n",
    "        self.Ur = self.add_weight(shape=(self.units, self.units),\n",
    "                                  initializer='orthogonal',\n",
    "                                  name='Ur')\n",
    "        self.Wz = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  name='Wz')\n",
    "        self.Uz = self.add_weight(shape=(self.units, self.units),\n",
    "                                  initializer='orthogonal',\n",
    "                                  name='Uz')\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 name='W')\n",
    "        self.U = self.add_weight(shape=(self.units, self.units),\n",
    "                                 initializer='orthogonal',\n",
    "                                 name='U')\n",
    "        self.bias_r = self.add_weight(shape=(self.units,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_r')\n",
    "        self.bias_z = self.add_weight(shape=(self.units,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_z')\n",
    "        self.bias_h = self.add_weight(shape=(self.units,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_h')\n",
    "\n",
    "    \n",
    "    def call(self, inputs, states):\n",
    "        prev_h = states[0]\n",
    "\n",
    "        r = tf.sigmoid(tf.matmul(inputs, self.Wr) + tf.matmul(prev_h, self.Ur) + self.bias_r)\n",
    "        \n",
    "        r_reduced = tf.reduce_mean(r, axis=-1, keepdims=True)  # Averaging across the units dimension\n",
    "        r_reduced = tf.tile(r_reduced, [1, inputs.shape[-1]])\n",
    "\n",
    "      \n",
    "        z = tf.sigmoid(tf.matmul(inputs * r_reduced, self.Wz) + tf.matmul(prev_h, self.Uz) + self.bias_z)\n",
    "\n",
    "        n = tf.tanh(tf.matmul(inputs, self.W) + tf.matmul(prev_h * r, self.U) + self.bias_h)\n",
    "\n",
    "        h = (1 - z) * prev_h + z * n\n",
    "\n",
    "        return h, [h]\n",
    "\n",
    "\n",
    "# Producing OGRU layer \n",
    "class OGRU(tf.keras.layers.RNN):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.cell = OGRUCell(units, **kwargs)\n",
    "        super(OGRU, self).__init__(self.cell, **kwargs)\n",
    "\n",
    "# OGRU layer with 80 units\n",
    "ogru_layer = OGRU(units=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU model\n",
    "gru_input = Input(shape = (backcandles, 2), name = 'gru_input')\n",
    "\n",
    "inputs = GRU(80, name='first_layer')(gru_input)\n",
    "\n",
    "inputs = Dense(1, name='dense_layer')(inputs)\n",
    "\n",
    "output = Activation('sigmoid', name = 'output')(inputs)\n",
    "\n",
    "model = Model(inputs = gru_input, outputs = output)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "history = model.fit(x=Z, y=y_final, epochs = 30, validation_data = (Z, y_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OGRU model\n",
    "ogru_input = Input(shape = (backcandles, 2), name = 'ogru_input')\n",
    "\n",
    "inputs = ogru_layer(ogru_input)\n",
    "\n",
    "inputs = Dense(1, name='dense_layer')(inputs)\n",
    "\n",
    "output = Activation('sigmoid', name = 'output')(inputs)\n",
    "\n",
    "model2 = Model(inputs = ogru_input, outputs = output)\n",
    "\n",
    "model2.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "history2 = model2.fit(x=Z, y=y_final, epochs = 30, validation_data = (Z, y_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gru plot\n",
    "history.history.keys() \n",
    "plt.plot(history.history['loss'], color = 'red', label = \"Training Loss\")\n",
    "plt.plot(history.history['accuracy'], color = 'royalblue', label = \"Training Accuracy\")\n",
    "plt.xlabel('Number of Epochs', fontsize = 15)\n",
    "plt.ylabel('Accuracy', fontsize = 15)\n",
    "plt.gca().tick_params(axis='x', labelsize=12)\n",
    "plt.gca().tick_params(axis='y', labelsize=12)\n",
    "plt.legend(fontsize= 12)\n",
    "#plt.savefig('GRU.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ogru plot\n",
    "history2.history.keys() \n",
    "plt.plot(history3.history['loss'], color = 'red', label = \"Training Loss\")\n",
    "plt.plot(history3.history['accuracy'], color = 'royalblue', label = \"Training Accuracy\")\n",
    "plt.xlabel('Number of Epochs', fontsize = 15)\n",
    "plt.ylabel('Accuracy', fontsize = 15)\n",
    "plt.gca().tick_params(axis='x', labelsize=12)\n",
    "plt.gca().tick_params(axis='y', labelsize=12)\n",
    "plt.legend(fontsize= 12)\n",
    "#plt.savefig('MyGRU.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data[[\"Return_Squared\", \"Hourly Volatility\"]]\n",
    "Y_test = test_data[\"target\"]\n",
    "test_dataset = test_data[[\"Return_Squared\", \"Hourly Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled = scaler.fit_transform(test_dataset)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = []\n",
    "\n",
    "backcandles = 10\n",
    "\n",
    "for j in range(2):\n",
    "    T.append([])\n",
    "    for i in range(backcandles, X_test_scaled.shape[0]):\n",
    "        T[j].append(X_test_scaled[i-backcandles:i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.moveaxis(T, [0], [2])\n",
    "T, yi_test = np.array(T), np.array(test_scaled[backcandles-1:, -1])\n",
    "y_final_test = np.reshape(yi_test,(len(yi_test),1))\n",
    "y_final_test = y_final_test[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gru\n",
    "test_predictions1 = model.predict(T)\n",
    "test_predicted_classes1 = (test_predictions1 > 0.5).astype(int)\n",
    "y_frame1 = test_data[[\"Date\", \"target\"]].tail(14393- backcandles +1)\n",
    "y_frame1 = y_frame1.iloc[:-1]\n",
    "y_frame1['predicted'] = test_predicted_classes1\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_frame1['predicted'], y_frame1['target'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ogru\n",
    "test_predictions2 = model3.predict(T)\n",
    "test_predicted_classes2 = (test_predictions2 > 0.5).astype(int)\n",
    "y_frame2 = test_data[[\"Date\", \"target\"]].tail(14393- backcandles +1)\n",
    "y_frame2 = y_frame2.iloc[:-1]\n",
    "y_frame2['predicted'] = test_predicted_classes2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_frame2['predicted'], y_frame2['target'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is an additional section which shows the performance for each year in the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "y_frame['Date'] = pd.to_datetime(y_frame['Date'])\n",
    "grouped_by_year = y_frame1.groupby(y_frame['Date'].dt.year)\n",
    "dataframes_by_year = {year: group for year, group in grouped_by_year}\n",
    "y_frame_2020 = dataframes_by_year[2020]\n",
    "y_frame_2021 = dataframes_by_year[2021]\n",
    "y_frame_2022 = dataframes_by_year[2022]\n",
    "y_frame_2023 = dataframes_by_year[2023]\n",
    "y_frame_2024 = dataframes_by_year[2024]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm2020 = confusion_matrix(y_frame_2020['predicted'], y_frame_2020['target'])\n",
    "cm2021 = confusion_matrix(y_frame_2021['predicted'], y_frame_2021['target'])\n",
    "cm2022 = confusion_matrix(y_frame_2022['predicted'], y_frame_2022['target'])\n",
    "cm2023 = confusion_matrix(y_frame_2023['predicted'], y_frame_2023['target'])\n",
    "cm2024 = confusion_matrix(y_frame_2024['predicted'], y_frame_2024['target'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(cm2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame and 'date_column' is the name of your date column\n",
    "y_frame['Date'] = pd.to_datetime(y_frame['Date'])\n",
    "grouped_by_year = y_frame2.groupby(y_frame['Date'].dt.year)\n",
    "dataframes_by_year = {year: group for year, group in grouped_by_year}\n",
    "y_frame_2020 = dataframes_by_year[2020]\n",
    "y_frame_2021 = dataframes_by_year[2021]\n",
    "y_frame_2022 = dataframes_by_year[2022]\n",
    "y_frame_2023 = dataframes_by_year[2023]\n",
    "y_frame_2024 = dataframes_by_year[2024]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm2020 = confusion_matrix(y_frame_2020['predicted'], y_frame_2020['target'])\n",
    "cm2021 = confusion_matrix(y_frame_2021['predicted'], y_frame_2021['target'])\n",
    "cm2022 = confusion_matrix(y_frame_2022['predicted'], y_frame_2022['target'])\n",
    "cm2023 = confusion_matrix(y_frame_2023['predicted'], y_frame_2023['target'])\n",
    "cm2024 = confusion_matrix(y_frame_2024['predicted'], y_frame_2024['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm2020)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss_model",
   "language": "python",
   "name": "diss_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
