{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, Activation, GRU, concatenate, Bidirectional \n",
    "from keras import Model\n",
    "from keras import optimizers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.read_csv('Daily Volatility Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_full[[\"Volume\", \"Return\", \"Return_Squared\", \"EMAF\", \"Daily Volatility\"]]\n",
    "Y = data_full[\"target\"]\n",
    "data_set = data_full[[\"Volume\", \"Return\", \"Return_Squared\", \"EMAF\", \"Daily Volatility\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitlimit = int(len(data_set)*0.8)\n",
    "training_features, test_data = data_set[:splitlimit], data_set[splitlimit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler1 = MinMaxScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "training_data_features_scaled = scaler1.fit_transform(training_features[[\"Volume\", \"Return\", \"Return_Squared\", \"EMAF\", \"Daily Volatility\"]])\n",
    "dataset_scaled = scaler2.fit_transform(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstructing training data for logistic regression\n",
    "Z = []\n",
    "\n",
    "backcandles = 10\n",
    "\n",
    "for j in range(5):\n",
    "    Z.append([])\n",
    "    for i in range(backcandles, training_data_features_scaled.shape[0]):\n",
    "        Z[j].append(training_data_features_scaled[i-backcandles:i, j])\n",
    "        \n",
    "Z = np.moveaxis(Z, [0], [2])        \n",
    "Z, yi = np.array(Z), np.array(dataset_scaled[backcandles-1:, -1])\n",
    "y_final = np.reshape(yi,(len(yi),1))\n",
    "y_final = y_final[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_features_scaled = scaler1.transform(test_data[[\"Volume\", \"Return\", \"Return_Squared\", \"EMAF\", \"Daily Volatility\"]])\n",
    "test_dataset_scaled = scaler2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstructing test data for logistic regression\n",
    "T = []\n",
    "\n",
    "backcandles = 10\n",
    "\n",
    "for j in range(5):\n",
    "    T.append([])\n",
    "    for i in range(backcandles, test_data_features_scaled.shape[0]):\n",
    "        T[j].append(test_data_features_scaled[i-backcandles:i, j])\n",
    "        \n",
    "        \n",
    "T = np.moveaxis(T, [0], [2])   \n",
    "T, yi_test = np.array(T), np.array(test_dataset_scaled[backcandles-1:, -1])\n",
    "y_final_test = np.reshape(yi_test,(len(yi_test),1))\n",
    "y_final_test = y_final_test[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "Z_flattened = Z.reshape(3807, -1)\n",
    "y_final_flattened = y_final.ravel()\n",
    "\n",
    "T_flattened = T.reshape(945, -1)\n",
    "y_final_test_flattened = y_final_test.ravel()\n",
    "\n",
    "\n",
    "model_LR = LogisticRegression()\n",
    "model_LR.fit(Z_flattened, y_final_flattened)\n",
    "\n",
    "LR_predictions = model_LR.predict(T_flattened)\n",
    "\n",
    "y_frame_LR = test_data[[\"target\"]].tail(955- backcandles +1)\n",
    "y_frame_LR = y_frame_LR.iloc[:-1]\n",
    "y_frame_LR['predicted'] = LR_predictions\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_frame_LR['predicted'], y_frame_LR['target'])\n",
    "print(cm)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_frame_LR['target'], y_frame_LR['predicted'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FFNN model\n",
    "input_shape = len(training_data_features_scaled[0, :]) \n",
    "\n",
    "ff_input = Input(shape=input_shape, name='ff_input')\n",
    "\n",
    "inputs = Dense(80, activation='relu', name='first_layer')(ff_input)  \n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(1, activation='sigmoid', name='output')(inputs)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=ff_input, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x=training_data_features_scaled, y=dataset_scaled[:, -1], epochs=50, validation_data=(training_data_features_scaled, dataset_scaled[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN_predictions = model.predict(test_data_features_scaled)\n",
    "FFNN_predictions_classes = (FFNN_predictions > 0.5).astype(int)\n",
    "y_frame_FFNN = test_dataset_scaled[:, -1]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"target\": y_frame_FFNN,\n",
    "    \"predicted\": FFNN_predictions_classes.ravel()  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(df['predicted'], df['target'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(df['target'], df['predicted'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss_model",
   "language": "python",
   "name": "diss_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
